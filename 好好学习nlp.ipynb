{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "好好学习nlp",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youk520/profile/blob/master/%E5%A5%BD%E5%A5%BD%E5%AD%A6%E4%B9%A0nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzzVyUt1NMV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRDn62Y9LSh",
        "colab_type": "code",
        "outputId": "49f73bc9-caec-4d7b-aae3-3035b1e4cfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"ningye\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"ebaa4dbd87f76fd6edd6c247ca44df98\" # key from the json file\n",
        "!kaggle competitions download -c hecmontrealdeeplearningcourse # api copied from kaggle\n",
        "\n",
        "if not os.path.exists('./text.csv'):\n",
        "  !unzip text.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading text.csv.zip to /content\n",
            "  0% 0.00/561k [00:00<?, ?B/s]\n",
            "100% 561k/561k [00:00<00:00, 38.2MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/69.5k [00:00<?, ?B/s]\n",
            "100% 69.5k/69.5k [00:00<00:00, 68.3MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/94.4k [00:00<?, ?B/s]\n",
            "100% 94.4k/94.4k [00:00<00:00, 84.5MB/s]\n",
            "Downloading sample.csv to /content\n",
            "  0% 0.00/94.5k [00:00<?, ?B/s]\n",
            "100% 94.5k/94.5k [00:00<00:00, 96.2MB/s]\n",
            "Downloading reference.csv to /content\n",
            "  0% 0.00/798k [00:00<?, ?B/s]\n",
            "100% 798k/798k [00:00<00:00, 94.8MB/s]\n",
            "Archive:  text.csv.zip\n",
            "  inflating: text.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_s4G4rp9ZyH",
        "colab_type": "code",
        "outputId": "d7712756-14b8-4982-9c75-f13175bf40a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "import pandas as pd   ##第一步先把id的数字转化成sequence\n",
        "\n",
        "text_csv = pd.read_csv('text.csv')\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "test_csv = pd.read_csv('test.csv')\n",
        "\n",
        "print(train_csv)\n",
        "\n",
        "title = text_csv.title.values\n",
        "title_id =  text_csv.id.values\n",
        "train_id = train_csv.id.values\n",
        "train_label = train_csv.label.values\n",
        "test_id = test_csv.id.values\n",
        "\n",
        "train_title = title[train_id]\n",
        "test_title = title[test_id]\n",
        "\n",
        "print(train_id)\n",
        "\n",
        "##print(len(train_title))\n",
        "##print(len(test_title))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id  label\n",
            "0          0      1\n",
            "1          3      1\n",
            "2          6      1\n",
            "3          8      0\n",
            "4          9      0\n",
            "...      ...    ...\n",
            "12774  25547      4\n",
            "12775  25548      3\n",
            "12776  25554      2\n",
            "12777  25555      2\n",
            "12778  25557      1\n",
            "\n",
            "[12779 rows x 2 columns]\n",
            "[    0     3     6 ... 25554 25555 25557]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3h-TjtF-NP-",
        "colab_type": "code",
        "outputId": "a0a551f8-0c60-441a-d620-00235427186b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "import numpy as np\n",
        "data_array = np.concatenate((title[train_id].reshape(-1,1),train_label.reshape(-1,1)),axis = 1)  ##把两列合在一起变成一个矩阵 seq与对应的label\n",
        "print(data_array.shape)\n",
        "print(data_array)\n",
        "\n",
        "data_index = np.arange(data_array.shape[0])\n",
        "print(data_index)\n",
        "##打乱顺序\n",
        "np.random.shuffle(data_index)\n",
        "\n",
        "print(data_index)\n",
        "##打乱顺序分数据集\n",
        "valid_index = data_index[:int(0.2*data_array.shape[0])]  ##分数据集 变成整形0.2validation\n",
        "\n",
        "train_index = data_index[int(0.8*data_array.shape[0]):] ##train 0.8\n",
        "\n",
        "print(valid_index)\n",
        "print(train_index)\n",
        "\n",
        "##把np array改成更csv\n",
        "np.savetxt('train_titles.csv', data_array[train_index], delimiter=',', fmt=['%s' , '%d'],header='title,label',comments='')\n",
        "np.savetxt('valid_titles.csv', data_array[valid_index], delimiter=',', fmt=['%s' , '%d'],header='title,label',comments='')\n",
        "train_csv = pd.read_csv('train_titles.csv')\n",
        "print(train_csv)\n",
        "\n",
        "np.savetxt('test_titles.csv', title[title_id], delimiter=',', fmt='%s')  ##第一列跟第二列是什么形式 title and label\n",
        "test_titles = pd.read_csv('test_titles.csv',header=None) ####test的title保存起来\n",
        "print(test_titles)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12779, 2)\n",
            "[['interactive visual exploration of neighbor based patterns in data streams'\n",
            "  1]\n",
            " ['relational division four algorithms and their performance' 1]\n",
            " ['simplifying xml schema effortless handling of nondeterministic regular expressions'\n",
            "  1]\n",
            " ...\n",
            " ['an analysis of transformational analogy general framework and complexity'\n",
            "  2]\n",
            " ['exploiting known taxonomies in learning overlapping concepts' 2]\n",
            " ['maintaining materialized views in distributed databases' 1]]\n",
            "[    0     1     2 ... 12776 12777 12778]\n",
            "[ 1514 11187  7586 ... 10976  8250  5215]\n",
            "[ 1514 11187  7586 ...  5992 11875  9496]\n",
            "[ 2783 11491   401 ... 10976  8250  5215]\n",
            "                                                  title  label\n",
            "0     deriving non redundant approximate association...      0\n",
            "1     enhancing cross language information retrieval...      0\n",
            "2                       classification enhanced ranking      0\n",
            "3     determining cylindrical shape from contour and...      2\n",
            "4           learning to connect language and perception      2\n",
            "...                                                 ...    ...\n",
            "2551  a semantic approach to usability in relational...      1\n",
            "2552  improving the cache locality of memory allocation      3\n",
            "2553  corpus based linguistic indicators for aspectu...      4\n",
            "2554  an analysis of the structural dynamic and temp...      1\n",
            "2555     semantic modeling of object oriented databases      1\n",
            "\n",
            "[2556 rows x 2 columns]\n",
            "                                                       0\n",
            "0      interactive visual exploration of neighbor bas...\n",
            "1      autodomainmine a graphical data mining system ...\n",
            "2      anipqo almost non intrusive parametric query o...\n",
            "3      relational division four algorithms and their ...\n",
            "4      selection and ranking of text from highly impe...\n",
            "...                                                  ...\n",
            "25556      dynamic typing in a statically typed language\n",
            "25557  maintaining materialized views in distributed ...\n",
            "25558     learning sparse metrics via linear programming\n",
            "25559             computer assisted reasoning with mizar\n",
            "25560  characterization of a large web site populatio...\n",
            "\n",
            "[25561 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkTcwwIGnY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "tokenize = lambda x: x.split()  ##把seq出现空格的就打散\n",
        "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
        "LABEL = Field(sequential=False, use_vocab=False)   ##nlp预备知识\n",
        "\n",
        "### load data splits\n",
        "train_dataset, valid_dataset = data.TabularDataset.splits(\n",
        "    path = os.getcwd(), format = 'csv', skip_header = True,\n",
        "    train='train_titles.csv', validation='valid_titles.csv',   ##函数需要csv格式\n",
        "    fields=[\n",
        "        ('title', TEXT),   ##title seq\n",
        "        ('label', LABEL)   \n",
        "    ]\n",
        ")\n",
        "\n",
        "# build dictionary\n",
        "TEXT.build_vocab(train_dataset)\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "\n",
        "# build iterators        不需要用到text iterator\n",
        "train_iter, val_iter = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset), \n",
        "    batch_size=32)   ##batch size 都可以改\n",
        "\n",
        "\n",
        "#for batch in train_iter:     ##iterater把seq 变成了vector 这事可以开始训练模型\n",
        " # print(batch.title)      ##不用print 这里只需要看效果\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE_Nz9_SiH-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##如何衔接起来 assignment1 做iterator\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch.text.cuda())\n",
        "        predictions = torch.max(logits, dim=-1)[1]\n",
        "        loss = criterion(logits, batch.label.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
        "        total_prediction += batch.label.size(0)\n",
        "    return total_loss / len(iterator), total_correct / total_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QscAwJVQMuJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate a model\n",
        "# DO NOT MODIFY\n",
        "def evaluate(model, iterator, criterion):  \n",
        "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            logits = model(batch.text.cuda())\n",
        "            predictions = torch.max(logits, dim=-1)[1]\n",
        "            loss = criterion(logits, batch.label.cuda())\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
        "            total_prediction += batch.label.size(0)\n",
        "    return total_loss / len(iterator), total_correct / total_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN2Aq279NA6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1\n",
        "        # add the layers required for sentiment analysis.\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, self.num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, label_size)\n",
        "\n",
        "    def zero_state(self, batch_size): \n",
        "        # implement the function, which returns an initial hidden state.\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        Init_state = (torch.zeros(self.num_layers, batch_size, self.hidden_dim).cuda(),torch.zeros(self.num_layers, batch_size, self.hidden_dim).cuda()) \n",
        "\n",
        "        return Init_state\n",
        "\n",
        "    def forward(self, text):\n",
        "        # implement the forward function of the model.\n",
        "        hidden = self.zero_state(text.size(0))\n",
        "        embedding = self.embedding(text)\n",
        "        lstm_out, (lstm_h, lstm_c)= self.lstm(embedding, hidden)\n",
        "        lstm_h = F.relu(torch.squeeze(lstm_h))\n",
        "        lstm_h = self.fc1(lstm_h)\n",
        "\n",
        "\n",
        "        return lstm_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4tHcamtNZQo",
        "colab_type": "code",
        "outputId": "da452a5c-7311-426e-8b3f-296160d1339b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)\n",
        "# tune the optimizer type and hyperparameters here\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "\n",
        "# train and test the model\n",
        "# DO NOT MODIFY\n",
        "best_valid_acc = 0.0\n",
        "best_state_dict = copy.deepcopy(model.state_dict())\n",
        "for epoch in range(20):\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print('Epoch {} | Train loss {:.3f} | Valid loss {:.3f} | Valid acc {:.3f}'.format(epoch, train_loss, valid_loss, valid_acc))\n",
        "\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_state_dict = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-11545bcf5dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbest_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bc56b4f6a111>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'text'"
          ]
        }
      ]
    }
  ]
}